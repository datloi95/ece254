\input{header.tex}

\begin{document}

\lecture{ 18 --- Concurrency \& Synchronization in POSIX }{\term}{Jeff Zarnett}

\section*{Concurrency with pthreads}

Earlier we saw pthreads and how we can use them to have multithreaded programs. There are, accordingly, pthread routines for working with mutual exclusion (mutexes).

The first function of note is \texttt{pthread\_mutex\_init} which is used to create a new mutex variable and returns it, with type \texttt{pthread\_mutex\_t}. It takes an optional parameter, the attributes (the details of which are not important at the moment, but relate mostly to priorities). We can initialize it using null, but there is also a syntactic shortcut to do static initialization if you do not want to set attributes~\cite{pthreads}: \\ \texttt{pthread\_mutex\_t mymutex = PTHREAD\_MUTEX\_INITIALIZER;} 

When created, by default, the mutex is unlocked. There are three methods related to using the mutex; two to lock it and one to unlock it, all of which take as a parameter the mutex to (un)lock. The unlock method, \texttt{pthread\_mutex\_unlock} is self-explanatory. The two kinds of lock are \texttt{pthread\_mutex\_lock}, which is blocking, and \texttt{pthread\_mutex\_trylock}, which is nonblocking. As expected, attempting to unlock a mutex that is not currently locked is an error, but it is also an error if one thread attempts to unlock a mutex owned by another thread~\cite{pthreads}.

To destroy a mutex, there is a method \texttt{pthread\_mutex\_destroy}. As expected, it cleans up a mutex and should be used when finished with it. If attributes were created with \texttt{pthread\_mutexattr\_init} they should be destroyed with \texttt{pthread\_mutexattr\_destroy}.

An attempt to destroy the mutex may fail if the mutex is currently locked. The specification says that destroying an unlocked mutex is okay, but attempting to destroy a locked one results in undefined behaviour. Undefined behaviour is, in the words of the internet, the worst thing ever: it means code might work some of the time or on some systems, but not others, or could work fine for a while and then break suddenly later when something else is changed\footnote{Sadly, the specifications for C and POSIX and many other things are riddled with these ``undefined behaviour'' situations and it causes programmers everywhere a great deal of stress and difficulty. Another example: reading from an uninitialized variable in C produces undefined behaviour too.}.

\subsection*{Condition Variables}
Condition variables are another way to achieve synchronization. Rather than designating critical areas and enforcing rules about how many threads may be in the critical area, a condition variable allows synchronization based on the value of the data. Instead of locking a mutex, checking a variable, and then unlocking the mutex, we could achieve the same goal without constantly polling. We can think of condition variables as ``events'' that occur (like interrupts from hardware when there is new data to read rather than polling to check periodically or constantly).

An event is similar to, but slightly different from, a counting semaphore. We have the option, when an event occurs, to signal either one thread waiting for that event to occur, or to broadcast (signal) to all threads waiting for the event~\cite{mte241}. It should be noted that if a thread signals a condition variable that an event has occurred, but no thread is waiting for that event, the event is ``lost''. 

Condition variables are also supported in pthreads (as you probably guessed from its appearance in this section). To create a \texttt{pthread\_cond\_t} (condition variable type), the function is \texttt{pthread\_cond\_init} and to destroy them, \texttt{pthread\_cond\_destroy}. As before, we can initialize them with attributes, and there are functions to create and destroy the attribute structures, too. 

Condition variables are a building block and must be paired with a mutex. To wait on a condition variable, the function \texttt{pthread\_cond\_wait} takes two parameters: the condition variable and the mutex. This routine should be called only while the mutex is locked. It will automatically release the mutex while it waits for the condition; when the condition is true then the mutex will be automatically locked again so the thread may proceed. The programmer then unlocks the mutex when the thread is finished with it~\cite{pthreads}. Obviously, failing to lock and unlock the mutex before and after using the condition variable, respectively, can result in problems.

In addition to the expected \texttt{pthread\_cond\_signal} function that signals a provided condition variable, there is also \texttt{pthread\_cond\_broadcast} that signals all threads waiting on that condition variable. Because an event that takes place when no thread is listening is simply lost, it is (almost always) a logical error to signal or broadcast on a condition variable before some thread is waiting on it.

\paragraph{Condition Variable Example.} Let us now examine a code example from~\cite{pthreads} that I've modified to make more compact:
\begin{lstlisting}[language=C]
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

#define NUM_THREADS  3
#define COUNT_LIMIT 12

int count = 0;
pthread_mutex_t count_mutex;
pthread_cond_t count_threshold_cv;

void* inc_count( void* arg ) {
  for (int i = 0; i < 10; i++ ) {
    pthread_mutex_lock( &count_mutex );
    count++;
    if ( count == COUNT_LIMIT ) {
      printf( "Condition Fulfilled!\n" );
      pthread_cond_signal( &count_threshold_cv );
      printf( "Sent signal.\n" );
    }
    pthread_mutex_unlock( &count_mutex );
  }
  pthread_exit( NULL );
}

void* watch_count( void *arg ) {
  pthread_mutex_lock( &count_mutex );  
  if ( count < COUNT_LIMIT ) {
    pthread_cond_wait( &count_threshold_cv, &count_mutex );
    printf( "Watcher has woken up.\n" );
    /* Do something useful here now that condition is fulfilled. */
  }
  pthread_mutex_unlock( &count_mutex );
  pthread_exit( NULL );
}

int main( int argc, char **argv ) {
  pthread_t threads[3];

  pthread_mutex_init( &count_mutex, NULL );
  pthread_cond_init ( &count_threshold_cv, NULL );

  pthread_create( &threads[0], NULL, watch_count, NULL );
  pthread_create( &threads[1], NULL, inc_count, NULL );
  pthread_create( &threads[2], NULL, inc_count, NULL );

  for ( int i = 0; i < NUM_THREADS; i++ ) {
    pthread_join(threads[i], NULL);
  }
  
  pthread_mutex_destroy( &count_mutex );
  pthread_cond_destroy( &count_threshold_cv );
  pthread_exit( NULL );
}
\end{lstlisting}

\subsection*{Monitors}
A condition variable can be used to create a \textit{monitor}, a higher level synchronization construct. Just as in object-oriented programming we package up data and functions inside a class to make errors less likely and to improve the design, when we use a monitor we are packaging up the shared data and operations on that data to avoid problems of synchronization and concurrency.

The objective of the monitor is to make it so that programmers do not need to code the synchronization part directly, making it less likely a programmer makes an error.

\begin{center}
\includegraphics[width=0.65\textwidth]{images/sync-monitor.png}\\
Conceptual view of a monitor~\cite{osc}.
\end{center}

Suppose there are processes $P$ and $Q$ and a condition variable $x$. If $P$ signals on $x$ when $Q$ is waiting, there are two things we can do: \textit{signal-and-wait}, where $P$ waits until $Q$ leaves the monitor (or another condition); or \textit{signal-and-continue} where $P$ continues and $Q$ waits for $P$ to leave the monitor or another condition. Signal-and-continue seems logical, in one sense: $P$ is already running, so why do the extra work to switch? On the other hand, by the time $P$ exits the monitor on its own, the condition $Q$ was waiting for might no longer be true. Some programming languages opt for a compromise: after signalling, $P$ must immediately leave the monitor~\cite{osc}.

The idea of monitors should be familiar to you if you have used Java synchronization constructs, notably the \texttt{synchronized} keyword. In Java we can declare a method to be \texttt{synchronized}, adding it after the access modifier keyword (public, private, etc) in the function definition, and then there is a lock created around that method. Only one thread can be inside that method at a time; if a second would like to call that method on the same instance, it will be placed in the entry set for the lock: a set of threads waiting for the lock to become available~\cite{osc}. 

\begin{verbatim}
public synchronized void doSomething() {
    // Synchronized area
}
\end{verbatim}


Note that in Java we can make a method \texttt{synchronized} or define a block as synchronized:

\begin{verbatim}
public void exampleMethod() {
    synchronized( object ) { // Lock must be acquired to enter this block
           // Critical section 
    } // Lock is automatically released.
}
\end{verbatim}

This sort of ``automatic'' locking and releasing is intended to simplify the process of writing multithreaded code and abstract away the details of mutual exclusion.

\section*{Concurrency Mechanisms in UNIX}

We have already mentioned the idea of pipes, shared memory, and signals as ways of having inter-process (or inter-thread) communication in UNIX. UNIX also has semaphores (not just in the pthreads). The semaphore in UNIX is a general semaphore, and maintains a few pieces of extra data~\cite{osi}:
\begin{itemize}
    \item The value of the semaphore.
    \item Process ID of the last process to operate it.
    \item Number of processes waiting on a value greater than the current value.
    \item Number of processes waiting for the value to be 0.
\end{itemize}

UNIX semaphores are technically created in sets of one or more. It is possible to manage semaphores en masse with a system call (\texttt{semctl}), which then performs the requested operation on all of the semaphores in the set. There is also a \texttt{sem\_op} system call that takes as an argument, a list of semaphore operations, each defined on one of the semaphores in the set. When \texttt{sem\_op} is used, the indicated operations are performed one at a time. The usage of \texttt{sem\_op} is as follows~\cite{osi}:
\begin{itemize}
    \item \texttt{sem\_op} positive: increment the value of the semaphore and wake up any processes waiting on that semaphore.
    \item \texttt{sem\_op} 0: if the semaphore value is currently 0, do nothing. Otherwise, block the process until the semaphore is 0, and increment the number of processes waiting for this to be 0.
    \item \texttt{sem\_op} negative and its absolute value less than or equal to the semaphore value: add \texttt{sem\_op} to the semaphore value, and if this makes the semaphore 0, wake up processes waiting for the semaphore to be 0.
    \item \texttt{sem\_op} negative and its absolute value greater than the semaphore value: block the process until the semaphore value is increased, and increment the number of processes waiting for the semaphore to be increased.
\end{itemize}

\subsection*{Linux Kernel Concurrency Mechanisms}

In addition to the standard UNIX concurrency mechanisms, Linux has concurrency mechanisms all its own, to provide concurrency in execution of kernel code. 

\paragraph{Atomic Operations.} The Linux kernel provides operations that are guaranteed to execute atomically, to avoid simple race conditions. In a uniprocessor system, the CPU cannot be interrupted until the operation is finished; in a multiprocessor system, the variable is locked until the operation is finished. There are two types of atomic operations: those that operate on integers and those that operate on a single bit in a bitmap. On some architectures, the atomic operations are translated into uninterruptible assembly instructions; on others, the memory bus is locked to ensure the operation is atomic~\cite{osi}.

Rather than just using an integer for atomic integer operations, there is a defined type \texttt{atomic\_t}, ensuring that only atomic operations can be used on this data type and that atomic operations can only be used on that data type. This prevents programming errors and hides architecture-specific implementation details.

The following table gives an overview of the atomic integer and bitmap operations, from~\cite{lkd}:

\begin{center}
\begin{tabular}{l|l}
	\textbf{Function} & \textbf{Description}\\\hline

	\texttt{ATOMIC\_INIT( int i )} & At declaration, initialize an \texttt{atomic\_t} to \texttt{i}\\\hline

\texttt{int atomic\_read( atomic\_t *v )} &  Read the integer value of \texttt{v}\\\hline

\texttt{void atomic\_set( atomic\_t *v, int i )} & Set \texttt{v} equal to \texttt{i}\\\hline

\texttt{void atomic\_add( int i, atomic\_t *v )} & Add \texttt{i} to \texttt{v}\\\hline

\texttt{void atomic\_sub( int i, atomic\_t *v )} & Subtract \texttt{i} from \texttt{v}\\\hline

\texttt{void atomic\_inc( atomic\_t *v )} & Add 1 to \texttt{v}\\\hline

\texttt{void atomic\_dec( atomic\_t *v )} & Subtract 1 from \texttt{v}\\\hline

\texttt{int atomic\_sub\_and\_test( int i, atomic\_t *v )} & Subtract \texttt{i} from \texttt{v}; return true if 0; otherwise false\\\hline

\texttt{int atomic\_add\_negative( int i, atomic\_t *v )} & Add \texttt{i} to \texttt{v}; return true if negative; otherwise false\\\hline

\texttt{int atomic\_dec\_and\_test( atomic\_t *v )} & Decrement \texttt{v} by 1; return true if 0; otherwise false\\\hline

\texttt{int atomic\_inc\_and\_test( atomic\_t *v )} & Increment \texttt{v} by 1; return true if 0; otherwise false\\ \hline\hline

\texttt{void set\_bit( int n, void *addr )} &  Set the $n^{th}$ bit starting from \texttt{addr}\\\hline

\texttt{void clear\_bit( int n, void *addr )} &  Clear the $n^{th}$ bit starting from \texttt{addr}\\\hline

\texttt{void change\_bit( int n, void *addr )} &  Flip the value of the $n^{th}$ bit starting from \texttt{addr}\\\hline

\texttt{int test\_and\_set\_bit( int n, void *addr )} &  Set $n^{th}$ bit starting from \texttt{addr}; return previous value\\\hline

\texttt{int test\_and\_clear\_bit( int n, void *addr )} &  Clear $n^{th}$ bit starting from \texttt{addr}; return previous value\\\hline

\texttt{int test\_and\_change\_bit( int n, void *addr )} &  Flip $n^{th}$ bit starting from \texttt{addr}; return previous value\\\hline

\texttt{int test\_bit( int n, void *addr )} &  Return value of $n^{th}$ bit starting from \texttt{addr}\\\hline

\end{tabular}
\end{center}

\paragraph{Spinlocks.}
Another common technique for protecting a critical section in Linux is the \textit{spinlock}. This is a handy way to implement constant checking to acquire a lock. Unlike semaphores where the process is blocked if it fails to acquire the lock, a thread will constantly try to acquire the lock. The implementation is an integer that is checked by a thread; if the value is 0, the thread can lock it (set the value to 1) and continue; if it is nonzero, it constantly checks the value until the value becomes 0. As you know, this is very inefficient; it would be better to let another thread execute, except in the circumstances where the amount of time waiting on the lock might be less than it would take to block the process, switch to another, and unblock it when the value changes~\cite{osi}.

\begin{verbatim}
spin_lock( &lock )
    /* Critical Section */
spin_unlock( & lock )
\end{verbatim}

In addition to the regular spinlock, there are \textit{reader-writer-spinlocks}. Like the readers-writers problem discussed earlier, the goal is to allow multiple readers but give exclusive access to a writer. This is implemented as a 24-bit reader counter and an unlock flag, with the meaning defined as follows~\cite{osi}.

\begin{center}
\begin{tabular}{l|l|l}
	\textbf{Counter} & \textbf{Flag} & \textbf{Interpretation}\\\hline
	0 & 1 & The spinlock is released and available. \\
	0 & 0 & The spinlock has been acquired for writing.\\
	$n$ ($n > 0$) & 0 & The spin lock has been acquired for reading by $n$ threads.\\
	$n$ ($n > 0$) & 1 & Invalid state.\\
\end{tabular}
\end{center}

There are further additional details related to use of spinlocks, which can of course be explored by reading the Linux kernel documentation.


\input{bibliography.tex}

\end{document}